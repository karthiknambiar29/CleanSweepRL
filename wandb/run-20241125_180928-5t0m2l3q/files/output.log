Episode 1/1000 - Reward: -1390.00, Loss: 0.0000
/home/protomate/CleanSweepRL/agent.py:139: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)
  "tether_length": torch.FloatTensor([state_dict["tether_length"]])
Episode 2/1000 - Reward: -1390.00, Loss: 6.2686
Episode 3/1000 - Reward: -1160.00, Loss: 4.6032
Episode 4/1000 - Reward: -1190.00, Loss: 4.4512
Episode 5/1000 - Reward: -1220.00, Loss: 4.1917
Episode 6/1000 - Reward: -1110.00, Loss: 3.8308
Episode 7/1000 - Reward: -1340.00, Loss: 3.7634
Episode 8/1000 - Reward: -1210.00, Loss: 3.5682
Episode 9/1000 - Reward: -1110.00, Loss: 3.4356
Episode 10/1000 - Reward: -1210.00, Loss: 3.4576
Episode 11/1000 - Reward: -1050.00, Loss: 4.7074
Episode 12/1000 - Reward: -1170.00, Loss: 3.8172
Episode 13/1000 - Reward: -1060.00, Loss: 3.6071
Episode 14/1000 - Reward: -1180.00, Loss: 3.4838
Episode 15/1000 - Reward: -1200.00, Loss: 3.5242
Episode 16/1000 - Reward: -1150.00, Loss: 3.3236
Episode 17/1000 - Reward: -950.00, Loss: 3.2271
Episode 18/1000 - Reward: -1120.00, Loss: 3.2343
Episode 19/1000 - Reward: -1030.00, Loss: 3.1079
/home/protomate/CleanSweepRL/env2.py:317: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
  self.fig, self.ax = plt.subplots()
Episode 20/1000 - Reward: -1030.00, Loss: 2.9512
Evaluation reward: -360.00
Episode 21/1000 - Reward: -940.00, Loss: 3.6403
Episode 22/1000 - Reward: -1050.00, Loss: 3.1186
Episode 23/1000 - Reward: -980.00, Loss: 3.0448
Episode 24/1000 - Reward: -1010.00, Loss: 2.6984
Episode 25/1000 - Reward: -880.00, Loss: 2.5558
Episode 26/1000 - Reward: -940.00, Loss: 2.5552
Episode 27/1000 - Reward: -990.00, Loss: 2.5362
Episode 28/1000 - Reward: -1090.00, Loss: 2.4564
Episode 29/1000 - Reward: -1000.00, Loss: 2.4089
Episode 30/1000 - Reward: -1020.00, Loss: 2.2703
Episode 31/1000 - Reward: -960.00, Loss: 3.0485
Episode 32/1000 - Reward: -1090.00, Loss: 2.7203
Episode 33/1000 - Reward: -930.00, Loss: 2.5300
Episode 34/1000 - Reward: -880.00, Loss: 2.4773
Episode 35/1000 - Reward: -750.00, Loss: 2.3491
Episode 36/1000 - Reward: -860.00, Loss: 2.3877
Episode 37/1000 - Reward: -900.00, Loss: 2.3579
Episode 38/1000 - Reward: -960.00, Loss: 2.2442
Episode 39/1000 - Reward: -860.00, Loss: 2.1582
Episode 40/1000 - Reward: -900.00, Loss: 2.2214
Evaluation reward: -380.00
Episode 41/1000 - Reward: -720.00, Loss: 2.8699
Episode 42/1000 - Reward: -790.00, Loss: 2.4064
Episode 43/1000 - Reward: -870.00, Loss: 2.2865
Episode 44/1000 - Reward: -740.00, Loss: 2.2803
Episode 45/1000 - Reward: -790.00, Loss: 2.2031
Episode 46/1000 - Reward: -710.00, Loss: 2.0351
Episode 47/1000 - Reward: -730.00, Loss: 2.0241
Episode 48/1000 - Reward: -740.00, Loss: 2.1312
Episode 49/1000 - Reward: -680.00, Loss: 1.9427
Episode 50/1000 - Reward: -640.00, Loss: 1.9116
Episode 51/1000 - Reward: -860.00, Loss: 2.5801
Episode 52/1000 - Reward: -620.00, Loss: 2.3942
Episode 53/1000 - Reward: -790.00, Loss: 2.2343
Episode 54/1000 - Reward: -760.00, Loss: 2.0861
Episode 55/1000 - Reward: -810.00, Loss: 2.0919
Episode 56/1000 - Reward: -880.00, Loss: 2.0237
Episode 57/1000 - Reward: -740.00, Loss: 1.9642
Episode 58/1000 - Reward: -660.00, Loss: 1.9096
Episode 59/1000 - Reward: -760.00, Loss: 1.9023
Episode 60/1000 - Reward: -710.00, Loss: 1.9352
Evaluation reward: -130.00
Episode 61/1000 - Reward: -700.00, Loss: 2.4336
Episode 62/1000 - Reward: -790.00, Loss: 2.2883
